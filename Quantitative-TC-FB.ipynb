{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load entity2id\n",
    "entity2id = {}\n",
    "with open(\"data/Freebase/FB15k/entity2id.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    e, idx = line.strip().split('\\t')\n",
    "    entity2id[e] = int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load relation2id\n",
    "relation2id = {}\n",
    "with open(\"data/Freebase/FB15k/relation2id.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    r, idx = line.strip().split('\\t')\n",
    "    relation2id[r] = int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load type2id\n",
    "type2id = {}\n",
    "with open(\"data/Freebase/FB15kET/type2id.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    t, idx = line.strip().split('\\t')\n",
    "    type2id[t] = int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training set of FB15k\n",
    "train_triplet = []\n",
    "with open('data/Freebase/FB15k/freebase_mtr100_mte100-train.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        h, l, t = line.strip().split(\"\\t\")\n",
    "        train_triplet.append((entity2id[h],relation2id[l],entity2id[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training set of FB15kET\n",
    "train_e2t = {}\n",
    "pair_train = 0 \n",
    "with open(\"data/Freebase/FB15kET/FB15k_Entity_Type_train.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        pair_train +=1\n",
    "        h, t = line.strip().split(\"\\t\")\n",
    "        if entity2id[h] not in train_e2t:\n",
    "            train_e2t[entity2id[h]] = []\n",
    "        if type2id[t] not in train_e2t[entity2id[h]]:\n",
    "            train_e2t[entity2id[h]].append(type2id[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation set of FB15kET\n",
    "dev_e2t = {}\n",
    "pair_dev =0\n",
    "with open(\"data/Freebase/FB15kET/FB15k_Entity_Type_valid_clean.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        pair_dev+=1\n",
    "        h, t = line.strip().split(\"\\t\")\n",
    "        if entity2id[h] not in dev_e2t:\n",
    "            dev_e2t[entity2id[h]] = []\n",
    "        if type2id[t] not in dev_e2t[entity2id[h]]:\n",
    "            dev_e2t[entity2id[h]].append(type2id[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set of FB15kET\n",
    "test_e2t = {}\n",
    "pair_test =0\n",
    "with open(\"data/Freebase/FB15kET/FB15k_Entity_Type_test_clean.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        pair_test+=1\n",
    "        h, t = line.strip().split(\"\\t\")\n",
    "        if entity2id[h] not in test_e2t:\n",
    "            test_e2t[entity2id[h]] = []\n",
    "        test_e2t[entity2id[h]].append(type2id[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse xxx2id\n",
    "id2entity = {entity2id[entity]:entity for entity in entity2id}\n",
    "id2type = {type2id[type_]: type_ for type_ in type2id}\n",
    "id2relation = {relation2id[relation]: relation for relation in relation2id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add self-loop relation\n",
    "self_rid = len(relation2id)\n",
    "self_triplet = []\n",
    "for entity in train_e2t:\n",
    "    self_triplet.append((entity, self_rid, entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count dictionary A, realize with p_h and p_t\n",
    "p_h = {}\n",
    "p_t = {}\n",
    "self_r = len(relation2id)\n",
    "for triplet in train_triplet+self_triplet:\n",
    "    h, r, t = triplet\n",
    "    if h in train_e2t and t in train_e2t:              # ensure the knowledge is known\n",
    "        for type_h in train_e2t[h]:\n",
    "            for type_t in train_e2t[t]:\n",
    "                if (type_h, r) not in p_h:\n",
    "                    p_h[(type_h, r)] = {}\n",
    "                if type_t not in p_h[(type_h, r)]:\n",
    "                    p_h[(type_h, r)][type_t] = 1\n",
    "                else:\n",
    "                    p_h[(type_h, r)][type_t] += 1\n",
    "                    \n",
    "                if (r, type_t) not in p_t:\n",
    "                    p_t[(r, type_t)] = {}\n",
    "                if type_h not in p_t[(r, type_t)]:\n",
    "                    p_t[(r, type_t)][type_h] = 1\n",
    "                else:\n",
    "                    p_t[(r, type_t)][type_h] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count dictionary Q_e and Q'_e\n",
    "h_count = {}\n",
    "t_count = {}\n",
    "for h_r in p_h:\n",
    "    type_h, r = h_r\n",
    "    count = sum(list(p_h[h_r].values()))\n",
    "    h_count[h_r] = count\n",
    "    p_h[h_r] = pd.DataFrame(list(p_h[h_r].values()),index=p_h[h_r].keys())\n",
    "\n",
    "for r_t in p_t:\n",
    "    r, type_t = r_t\n",
    "    count = sum(list(p_t[r_t].values()))\n",
    "    t_count[r_t] = count\n",
    "    p_t[r_t] = pd.DataFrame(list(p_t[r_t].values()),index=p_t[r_t].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "def evaluation(data_name='dev'):\n",
    "    mr = mrr = hit10 = hit3 =hit1 = 0\n",
    "    fmr = fmrr = fhit10 = fhit3 = fhit1 = 0\n",
    "    if data_name == 'dev':\n",
    "        data_set = dev_e2t.copy()\n",
    "    elif data_name == 'test':\n",
    "        data_set = test_e2t.copy()\n",
    "    for entity_ in data_set:\n",
    "        type_arg = np.argsort(-entity2type[entity_])\n",
    "        test_rank_list = []\n",
    "        train_rank_list = []\n",
    "        valid_rank_list = []\n",
    "        if entity_ in test_e2t:\n",
    "            for type_label in test_e2t[entity_]:\n",
    "                rank = (type_arg==type_label).nonzero()[0].item()+1\n",
    "                test_rank_list.append(rank)\n",
    "        if entity_ in dev_e2t:\n",
    "            for type_label in dev_e2t[entity_]:\n",
    "                rank = (type_arg==type_label).nonzero()[0].item()+1\n",
    "                valid_rank_list.append(rank)\n",
    "        if entity_ in train_e2t:\n",
    "            for type_label in train_e2t[entity_]:\n",
    "                rank = (type_arg==type_label).nonzero()[0].item()+1\n",
    "                train_rank_list.append(rank)\n",
    "        rank_list = train_rank_list + test_rank_list + valid_rank_list\n",
    "        rank_list.sort()\n",
    "        \n",
    "        if data_name == 'dev':\n",
    "            target_rank_list = valid_rank_list.copy()\n",
    "        elif data_name == 'test':\n",
    "            target_rank_list = test_rank_list.copy()\n",
    "        \n",
    "        for i, rank in enumerate(target_rank_list):\n",
    "            #rank is ’raw‘ rank\n",
    "            #raw-index is the rank of all correct\n",
    "            #rank - raw_index is filt rank\n",
    "            raw_index = rank_list.index(rank)\n",
    "            frank = rank - raw_index\n",
    "\n",
    "            #if rank == raw_index  frank shoud be 1\n",
    "            if frank <= 0:\n",
    "                frank = 1\n",
    "\n",
    "            fmr += frank\n",
    "            fmrr += 1.0/frank\n",
    "            if frank <=10:\n",
    "                fhit10 += 1\n",
    "            if frank <=3:\n",
    "                fhit3 += 1\n",
    "            if frank <= 1:\n",
    "                fhit1 += 1\n",
    "    \n",
    "    num_of_e2t = 0\n",
    "    for i in data_set:\n",
    "        num_of_e2t += len(data_set[i])\n",
    "        \n",
    "    return fmrr/num_of_e2t, fhit1/num_of_e2t, fhit3/num_of_e2t, fhit10/num_of_e2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB15k: 483142 triples\n",
      "FB15kET: 136618 train pairs,  15749 valid pairs, 15780 test pairs\n"
     ]
    }
   ],
   "source": [
    "# initialize M entity-type\n",
    "entity2type = np.zeros((len(entity2id), len(type2id)))\n",
    "print(\"FB15k:\",len(train_triplet),\"triples\")\n",
    "print(\"FB15kET:\",pair_train,\"train pairs, \", pair_dev,\"valid pairs,\", pair_test, \"test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 triples have been calculated\n",
      "20000 triples have been calculated\n",
      "30000 triples have been calculated\n",
      "40000 triples have been calculated\n",
      "50000 triples have been calculated\n",
      "60000 triples have been calculated\n",
      "70000 triples have been calculated\n",
      "80000 triples have been calculated\n",
      "90000 triples have been calculated\n",
      "100000 triples have been calculated\n",
      "110000 triples have been calculated\n",
      "120000 triples have been calculated\n",
      "130000 triples have been calculated\n",
      "140000 triples have been calculated\n",
      "150000 triples have been calculated\n",
      "160000 triples have been calculated\n",
      "170000 triples have been calculated\n",
      "180000 triples have been calculated\n",
      "190000 triples have been calculated\n",
      "200000 triples have been calculated\n",
      "210000 triples have been calculated\n",
      "220000 triples have been calculated\n",
      "230000 triples have been calculated\n",
      "240000 triples have been calculated\n",
      "250000 triples have been calculated\n",
      "260000 triples have been calculated\n",
      "270000 triples have been calculated\n",
      "280000 triples have been calculated\n",
      "290000 triples have been calculated\n",
      "300000 triples have been calculated\n",
      "310000 triples have been calculated\n",
      "320000 triples have been calculated\n",
      "330000 triples have been calculated\n",
      "340000 triples have been calculated\n",
      "350000 triples have been calculated\n",
      "360000 triples have been calculated\n",
      "370000 triples have been calculated\n",
      "380000 triples have been calculated\n",
      "390000 triples have been calculated\n",
      "400000 triples have been calculated\n",
      "410000 triples have been calculated\n",
      "420000 triples have been calculated\n",
      "430000 triples have been calculated\n",
      "440000 triples have been calculated\n",
      "450000 triples have been calculated\n",
      "460000 triples have been calculated\n",
      "470000 triples have been calculated\n",
      "480000 triples have been calculated\n",
      "490000 triples have been calculated\n"
     ]
    }
   ],
   "source": [
    "omega = 3\n",
    "for i, triplet in enumerate(train_triplet+self_triplet):\n",
    "    h, r, t = triplet\n",
    "    if r == self_rid:\n",
    "        r_weight = omega\n",
    "    else:\n",
    "        r_weight = 1\n",
    "    if h in train_e2t:\n",
    "        normalize_h = len(train_e2t[h])\n",
    "        for type_h in train_e2t[h]:\n",
    "            if (type_h, r) in p_h:\n",
    "                df = p_h[(type_h, r)]\n",
    "                t_key = list(df.index)\n",
    "                t_value = df.values.reshape(-1)\n",
    "                entity2type[t][t_key] += t_value/h_count[(type_h, r)]/normalize_h * r_weight\n",
    "\n",
    "    if t in train_e2t:\n",
    "        normalize_t = len(train_e2t[t])\n",
    "        for type_t in train_e2t[t]:\n",
    "            if (r, type_t) in p_t:\n",
    "                df = p_t[(r, type_t)]\n",
    "                h_key = list(df.index)\n",
    "                h_value = df.values.reshape(-1)\n",
    "                entity2type[h][h_key] += h_value/t_count[(r, type_t)]/normalize_t * r_weight\n",
    "\n",
    "    if (i+1)%10000 == 0:\n",
    "        print(i+1,\"triples have been calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results of validation： (0.5659360550033553, 0.4707600482570322, 0.6088640548606261, 0.7573814210426059)\n",
      "results of test： (0.5681619527719328, 0.470595690747782, 0.6145754119138149, 0.7641951837769329)\n"
     ]
    }
   ],
   "source": [
    "dev_evaluation = evaluation('dev')\n",
    "test_evaluation = evaluation('test')\n",
    "        \n",
    "print(\"results of validation：\",dev_evaluation)\n",
    "print(\"results of test：\",test_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "G = nx.MultiDiGraph()\n",
    "for triple in train_triplet:\n",
    "    h, r, t = triple\n",
    "    G.add_edge(h,t,key=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity_id: 9670 ,mid_id: /m/03d63lb ,entity: Raza Murad\n",
      "[(6802, 9670)]\n",
      "[(9670, 13605), (9670, 6802), (9670, 3492)]\n"
     ]
    }
   ],
   "source": [
    "print(\"entity_id:\",9670,\",mid_id:\",id2entity[9670],\",entity: Raza Murad\")   #'Raza Murad'\n",
    "print(G.in_edges(9670))                 #in neighbor\n",
    "print(G.out_edges(9670))                #out neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity_id: 6802 ,mid_id: /m/09fn1w ,entity: Jodhaa Akbar\n",
      "entity_id: 13605 ,mid_id: /m/03rk0 ,entity: India\n",
      "entity_id: 3492 ,mid_id: /m/02hrh1q ,entity: actor\n"
     ]
    }
   ],
   "source": [
    "print(\"entity_id:\",6802,\",mid_id:\",id2entity[6802],\",entity: Jodhaa Akbar\")   #'Jodhaa Akbar'\n",
    "print(\"entity_id:\",13605,\",mid_id:\",id2entity[13605],\",entity: India\")   #'India'\n",
    "print(\"entity_id:\",3492,\",mid_id:\",id2entity[3492],\",entity: actor\")   #'actor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1196: {}}\n",
      "{875: {}}\n",
      "{669: {}}\n",
      "{22: {}}\n"
     ]
    }
   ],
   "source": [
    "print(G[6802][9670])\n",
    "print(G[9670][13605])\n",
    "print(G[9670][6802])\n",
    "print(G[9670][3492])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Jodhaa Akbar, /film/film/starring./film/performance/actor ,Raza Murad)\n",
      "(Raza Murad, /film/actor/film./film/performance/film ,Jodhaa Akbar)\n",
      "(Raza Murad, /people/person/nationality ,India)\n",
      "(Raza Murad, /people/person/profession ,actor)\n"
     ]
    }
   ],
   "source": [
    "print(\"(Jodhaa Akbar,\",id2relation[1196],\",Raza Murad)\")\n",
    "print(\"(Raza Murad,\",id2relation[669],\",Jodhaa Akbar)\")\n",
    "print(\"(Raza Murad,\",id2relation[875],\",India)\")\n",
    "print(\"(Raza Murad,\",id2relation[22],\",actor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845 /award/award_nominated_work\n",
      "3718 /base/ovguide/bollywood_films\n",
      "3364 /base/filmfareawards/topic\n",
      "580 /film/film\n",
      "1967 /media_common/netflix_title\n",
      "841 /base/ovguide/topic\n",
      "3827 /award/award_winning_work\n"
     ]
    }
   ],
   "source": [
    "# Jodhaa Akbar ETIs\n",
    "for type_ in train_e2t[6802]:\n",
    "    print(type_, id2type[type_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "2039 /people/person\n",
      "3321 /award/award_nominee\n",
      "3572 /film/actor\n",
      "test\n",
      "131 /influence/influence_node\n"
     ]
    }
   ],
   "source": [
    "# Raza Murad ETIs\n",
    "print('train')\n",
    "for type_ in train_e2t[9670]:\n",
    "    print(type_,id2type[type_])\n",
    "print('test')\n",
    "for type_ in test_e2t[9670]:\n",
    "    print(type_,id2type[type_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the conditional probability of triple \n",
    "#(Jodhaa Akbar, /film/film/starring./film/performance/actor, -)\n",
    "temp_array =np.zeros(len(type2id))\n",
    "for type_ in train_e2t[6802]:\n",
    "    query = (type_, 1196)\n",
    "    for ans in range(len(type2id)):\n",
    "        if query in h_count:\n",
    "            if ans in p_h[query].index:\n",
    "                temp_array[ans] += p_h[query].loc[ans].item()/h_count[query]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see /people/person(2039), /film/actor(3572), /award/award_nominee(3321), /tv/tv_actor(130) ans so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2039 3572 3321  130  131 2019  736 3385 2948 2233]\n"
     ]
    }
   ],
   "source": [
    "print(np.argsort(-temp_array)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
