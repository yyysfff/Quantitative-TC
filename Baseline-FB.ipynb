{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load entity2id\n",
    "entity2id = {}\n",
    "with open(\"data/Freebase/FB15k/entity2id.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    e, idx = line.strip().split('\\t')\n",
    "    entity2id[e] = int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load relation2id\n",
    "relation2id = {}\n",
    "with open(\"data/Freebase/FB15k/relation2id.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    r, idx = line.strip().split('\\t')\n",
    "    relation2id[r] = int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load type2id\n",
    "type2id = {}\n",
    "with open(\"data/Freebase/FB15kET/type2id.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    t, idx = line.strip().split('\\t')\n",
    "    type2id[t] = int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training set of FB15k\n",
    "train_triplet = []\n",
    "with open('data/Freebase/FB15k/freebase_mtr100_mte100-train.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        h, l, t = line.strip().split(\"\\t\")\n",
    "        train_triplet.append((entity2id[h],relation2id[l],entity2id[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training set of FB15kET\n",
    "train_e2t = {}\n",
    "pair_train = 0 \n",
    "with open(\"data/Freebase/FB15kET/FB15k_Entity_Type_train.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        pair_train +=1\n",
    "        h, t = line.strip().split(\"\\t\")\n",
    "        if entity2id[h] not in train_e2t:\n",
    "            train_e2t[entity2id[h]] = []\n",
    "        if type2id[t] not in train_e2t[entity2id[h]]:\n",
    "            train_e2t[entity2id[h]].append(type2id[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation set of FB15kET\n",
    "dev_e2t = {}\n",
    "pair_dev =0\n",
    "with open(\"data/Freebase/FB15kET/FB15k_Entity_Type_valid_clean.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        pair_dev+=1\n",
    "        h, t = line.strip().split(\"\\t\")\n",
    "        if entity2id[h] not in dev_e2t:\n",
    "            dev_e2t[entity2id[h]] = []\n",
    "        if type2id[t] not in dev_e2t[entity2id[h]]:\n",
    "            dev_e2t[entity2id[h]].append(type2id[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set of FB15kET\n",
    "test_e2t = {}\n",
    "pair_test =0\n",
    "with open(\"data/Freebase/FB15kET/FB15k_Entity_Type_test_clean.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        pair_test+=1\n",
    "        h, t = line.strip().split(\"\\t\")\n",
    "        if entity2id[h] not in test_e2t:\n",
    "            test_e2t[entity2id[h]] = []\n",
    "        test_e2t[entity2id[h]].append(type2id[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB15k: 483142 triples\n",
      "FB15kET: 136618 train pairs,  15749 valid pairs, 15780 test pairs\n"
     ]
    }
   ],
   "source": [
    "print(\"FB15k:\",len(train_triplet),\"triples\")\n",
    "print(\"FB15kET:\",pair_train,\"train pairs, \", pair_dev,\"valid pairs,\", pair_test, \"test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializa M_e2r and M_r2t\n",
    "relation_head_type =np.zeros((len(relation2id), len(type2id)))\n",
    "relation_tail_type =np.zeros((len(relation2id), len(type2id)))\n",
    "head_link_relation =np.zeros((len(entity2id), len(relation2id)))\n",
    "tail_link_relation =np.zeros((len(entity2id), len(relation2id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for triplet in train_triplet:\n",
    "    h, r, t = triplet\n",
    "    # get ETIs of h\n",
    "    if h in train_e2t:\n",
    "        head_type = train_e2t[h]\n",
    "        relation_head_type[r][head_type] += 1\n",
    "    \n",
    "    if t in train_e2t:\n",
    "        tail_type = train_e2t[t] \n",
    "        relation_tail_type[r][tail_type] += 1\n",
    "        \n",
    "    head_link_relation[h][r] += 1\n",
    "    tail_link_relation[t][r] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2relation = np.concatenate((head_link_relation, tail_link_relation), axis=1)\n",
    "relation2type = np.concatenate((relation_head_type, relation_tail_type), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate M\n",
    "entity2type = np.matmul(entity2relation, relation2type)\n",
    "e2t_arg = np.argsort(-entity2type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "def evaluation(data_name='dev'):\n",
    "    mr = mrr = hit10 = hit3 =hit1 = 0\n",
    "    fmr = fmrr = fhit10 = fhit3 = fhit1 = 0\n",
    "    if data_name == 'dev':\n",
    "        data_set = dev_e2t.copy()\n",
    "    elif data_name == 'test':\n",
    "        data_set = test_e2t.copy()\n",
    "    for entity_ in data_set:\n",
    "        type_arg = np.argsort(-entity2type[entity_])\n",
    "        test_rank_list = []\n",
    "        train_rank_list = []\n",
    "        valid_rank_list = []\n",
    "        if entity_ in test_e2t:\n",
    "            for type_label in test_e2t[entity_]:\n",
    "                rank = (type_arg==type_label).nonzero()[0].item()+1\n",
    "                test_rank_list.append(rank)\n",
    "        if entity_ in dev_e2t:\n",
    "            for type_label in dev_e2t[entity_]:\n",
    "                rank = (type_arg==type_label).nonzero()[0].item()+1\n",
    "                valid_rank_list.append(rank)\n",
    "        if entity_ in train_e2t:\n",
    "            for type_label in train_e2t[entity_]:\n",
    "                rank = (type_arg==type_label).nonzero()[0].item()+1\n",
    "                train_rank_list.append(rank)\n",
    "        rank_list = train_rank_list + test_rank_list + valid_rank_list\n",
    "        rank_list.sort()\n",
    "        \n",
    "        if data_name == 'dev':\n",
    "            target_rank_list = valid_rank_list.copy()\n",
    "        elif data_name == 'test':\n",
    "            target_rank_list = test_rank_list.copy()\n",
    "        \n",
    "        for i, rank in enumerate(target_rank_list):\n",
    "            #rank is ’raw‘ rank\n",
    "            #raw-index is the rank of all correct\n",
    "            #rank - raw_index is filt rank\n",
    "            raw_index = rank_list.index(rank)\n",
    "            frank = rank - raw_index\n",
    "\n",
    "            #if rank == raw_index  frank shoud be 1\n",
    "            if frank <= 0:\n",
    "                frank = 1\n",
    "\n",
    "            fmr += frank\n",
    "            fmrr += 1.0/frank\n",
    "            if frank <=10:\n",
    "                fhit10 += 1\n",
    "            if frank <=3:\n",
    "                fhit3 += 1\n",
    "            if frank <= 1:\n",
    "                fhit1 += 1\n",
    "    \n",
    "    num_of_e2t = 0\n",
    "    for i in data_set:\n",
    "        num_of_e2t += len(data_set[i])\n",
    "        \n",
    "    return fmrr/num_of_e2t, fhit1/num_of_e2t, fhit3/num_of_e2t, fhit10/num_of_e2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results of validation： (0.49361499121152574, 0.3999619023430059, 0.5352085846720427, 0.6762334116451838)\n",
      "results of test： (0.4957710207982826, 0.4007604562737643, 0.5370088719898606, 0.6818124207858048)\n"
     ]
    }
   ],
   "source": [
    "dev_evaluation = evaluation('dev')\n",
    "test_evaluation = evaluation('test')\n",
    "        \n",
    "print(\"results of validation：\",dev_evaluation)\n",
    "print(\"results of test：\",test_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
