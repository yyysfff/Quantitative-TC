{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load entity2id\n",
    "entity2id = {}\n",
    "with open(\"data/YAGO/YAGO43k/entity2id.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    e, idx = line.strip().split('\\t')\n",
    "    entity2id[e] = int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load relation2id\n",
    "relation2id = {}\n",
    "with open(\"data/YAGO/YAGO43k/relation2id.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    r, idx = line.strip().split('\\t')\n",
    "    relation2id[r] = int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load type2id\n",
    "type2id = {}\n",
    "with open(\"data/YAGO/YAGO43kET/type2id.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    t, idx = line.strip().split('\\t')\n",
    "    type2id[t] = int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training set of YAGO43k\n",
    "train_triplet = []\n",
    "with open('data/YAGO/YAGO43k/YAGO43k_name_train.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        h, l, t = line.strip().split(\"\\t\")\n",
    "        train_triplet.append((entity2id[h],relation2id[l],entity2id[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training set of YAGO43kET\n",
    "train_e2t = {}\n",
    "pair_train = 0 \n",
    "with open(\"data/YAGO/YAGO43kET/YAGO43k_Entity_Type_train_clean.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        pair_train +=1\n",
    "        h, t = line.strip().split(\"\\t\")\n",
    "        if entity2id[h] not in train_e2t:\n",
    "            train_e2t[entity2id[h]] = []\n",
    "        if type2id[t] not in train_e2t[entity2id[h]]:\n",
    "            train_e2t[entity2id[h]].append(type2id[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation set of YAGO43kET\n",
    "dev_e2t = {}\n",
    "pair_dev =0\n",
    "with open(\"data/YAGO/YAGO43kET/YAGO43k_Entity_Type_valid_clean_clean.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        pair_dev+=1\n",
    "        h, t = line.strip().split(\"\\t\")\n",
    "        if entity2id[h] not in dev_e2t:\n",
    "            dev_e2t[entity2id[h]] = []\n",
    "        if type2id[t] not in dev_e2t[entity2id[h]]:\n",
    "            dev_e2t[entity2id[h]].append(type2id[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set of YAGO43kET\n",
    "test_e2t = {}\n",
    "pair_test =0\n",
    "with open(\"data/YAGO/YAGO43kET/YAGO43k_Entity_Type_test_clean_clean.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        pair_test+=1\n",
    "        h, t = line.strip().split(\"\\t\")\n",
    "        if entity2id[h] not in test_e2t:\n",
    "            test_e2t[entity2id[h]] = []\n",
    "        test_e2t[entity2id[h]].append(type2id[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse xxx2id\n",
    "id2entity = {entity2id[entity]:entity for entity in entity2id}\n",
    "id2type = {type2id[type_]: type_ for type_ in type2id}\n",
    "id2relation = {relation2id[relation]: relation for relation in relation2id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add self-loop relation\n",
    "self_rid = len(relation2id)\n",
    "self_triplet = []\n",
    "for entity in train_e2t:\n",
    "    self_triplet.append((entity, self_rid, entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count dictionary A, realize with p_h and p_t\n",
    "p_h = {}\n",
    "p_t = {}\n",
    "self_r = len(relation2id)\n",
    "for triplet in train_triplet+self_triplet:\n",
    "    h, r, t = triplet\n",
    "    if h in train_e2t and t in train_e2t:              # ensure the knowledge is known\n",
    "        for type_h in train_e2t[h]:\n",
    "            for type_t in train_e2t[t]:\n",
    "                if (type_h, r) not in p_h:\n",
    "                    p_h[(type_h, r)] = {}\n",
    "                if type_t not in p_h[(type_h, r)]:\n",
    "                    p_h[(type_h, r)][type_t] = 1\n",
    "                else:\n",
    "                    p_h[(type_h, r)][type_t] += 1\n",
    "                    \n",
    "                if (r, type_t) not in p_t:\n",
    "                    p_t[(r, type_t)] = {}\n",
    "                if type_h not in p_t[(r, type_t)]:\n",
    "                    p_t[(r, type_t)][type_h] = 1\n",
    "                else:\n",
    "                    p_t[(r, type_t)][type_h] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count dictionary Q_e and Q'_e\n",
    "h_count = {}\n",
    "t_count = {}\n",
    "for h_r in p_h:\n",
    "    type_h, r = h_r\n",
    "    count = sum(list(p_h[h_r].values()))\n",
    "    h_count[h_r] = count\n",
    "    p_h[h_r] = pd.DataFrame(list(p_h[h_r].values()),index=p_h[h_r].keys())\n",
    "\n",
    "for r_t in p_t:\n",
    "    r, type_t = r_t\n",
    "    count = sum(list(p_t[r_t].values()))\n",
    "    t_count[r_t] = count\n",
    "    p_t[r_t] = pd.DataFrame(list(p_t[r_t].values()),index=p_t[r_t].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "def evaluation(data_name='dev'):\n",
    "    mr = mrr = hit10 = hit3 =hit1 = 0\n",
    "    fmr = fmrr = fhit10 = fhit3 = fhit1 = 0\n",
    "    if data_name == 'dev':\n",
    "        data_set = dev_e2t.copy()\n",
    "    elif data_name == 'test':\n",
    "        data_set = test_e2t.copy()\n",
    "    for entity_ in data_set:\n",
    "        type_arg = np.argsort(-entity2type[entity_])\n",
    "        test_rank_list = []\n",
    "        train_rank_list = []\n",
    "        valid_rank_list = []\n",
    "        if entity_ in test_e2t:\n",
    "            for type_label in test_e2t[entity_]:\n",
    "                rank = (type_arg==type_label).nonzero()[0].item()+1\n",
    "                test_rank_list.append(rank)\n",
    "        if entity_ in dev_e2t:\n",
    "            for type_label in dev_e2t[entity_]:\n",
    "                rank = (type_arg==type_label).nonzero()[0].item()+1\n",
    "                valid_rank_list.append(rank)\n",
    "        if entity_ in train_e2t:\n",
    "            for type_label in train_e2t[entity_]:\n",
    "                rank = (type_arg==type_label).nonzero()[0].item()+1\n",
    "                train_rank_list.append(rank)\n",
    "        rank_list = train_rank_list + test_rank_list + valid_rank_list\n",
    "        rank_list.sort()\n",
    "        \n",
    "        if data_name == 'dev':\n",
    "            target_rank_list = valid_rank_list.copy()\n",
    "        elif data_name == 'test':\n",
    "            target_rank_list = test_rank_list.copy()\n",
    "        \n",
    "        for i, rank in enumerate(target_rank_list):\n",
    "            #rank is ’raw‘ rank\n",
    "            #raw-index is the rank of all correct\n",
    "            #rank - raw_index is filt rank\n",
    "            raw_index = rank_list.index(rank)\n",
    "            frank = rank - raw_index\n",
    "\n",
    "            #if rank == raw_index  frank shoud be 1\n",
    "            if frank <= 0:\n",
    "                frank = 1\n",
    "\n",
    "            fmr += frank\n",
    "            fmrr += 1.0/frank\n",
    "            if frank <=10:\n",
    "                fhit10 += 1\n",
    "            if frank <=3:\n",
    "                fhit3 += 1\n",
    "            if frank <= 1:\n",
    "                fhit1 += 1\n",
    "    \n",
    "    num_of_e2t = 0\n",
    "    for i in data_set:\n",
    "        num_of_e2t += len(data_set[i])\n",
    "        \n",
    "    return fmrr/num_of_e2t, fhit1/num_of_e2t, fhit3/num_of_e2t, fhit10/num_of_e2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAGO43k: 331687 triples\n",
      "YAGO43kET: 375853 train pairs,  42739 valid pairs, 42750 test pairs\n"
     ]
    }
   ],
   "source": [
    "# initialize M entity-type\n",
    "entity2type = np.zeros((len(entity2id), len(type2id)),dtype=np.float16)\n",
    "print(\"YAGO43k:\",len(train_triplet),\"triples\")\n",
    "print(\"YAGO43kET:\",pair_train,\"train pairs, \", pair_dev,\"valid pairs,\", pair_test, \"test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 triples have been calculated\n",
      "20000 triples have been calculated\n",
      "30000 triples have been calculated\n",
      "40000 triples have been calculated\n",
      "50000 triples have been calculated\n",
      "60000 triples have been calculated\n",
      "70000 triples have been calculated\n",
      "80000 triples have been calculated\n",
      "90000 triples have been calculated\n",
      "100000 triples have been calculated\n",
      "110000 triples have been calculated\n",
      "120000 triples have been calculated\n",
      "130000 triples have been calculated\n",
      "140000 triples have been calculated\n",
      "150000 triples have been calculated\n",
      "160000 triples have been calculated\n",
      "170000 triples have been calculated\n",
      "180000 triples have been calculated\n",
      "190000 triples have been calculated\n",
      "200000 triples have been calculated\n",
      "210000 triples have been calculated\n",
      "220000 triples have been calculated\n",
      "230000 triples have been calculated\n",
      "240000 triples have been calculated\n",
      "250000 triples have been calculated\n",
      "260000 triples have been calculated\n",
      "270000 triples have been calculated\n",
      "280000 triples have been calculated\n",
      "290000 triples have been calculated\n",
      "300000 triples have been calculated\n",
      "310000 triples have been calculated\n",
      "320000 triples have been calculated\n",
      "330000 triples have been calculated\n",
      "340000 triples have been calculated\n",
      "350000 triples have been calculated\n",
      "360000 triples have been calculated\n",
      "370000 triples have been calculated\n"
     ]
    }
   ],
   "source": [
    "omega = 100\n",
    "for i, triplet in enumerate(train_triplet+self_triplet):\n",
    "    h, r, t = triplet\n",
    "    if r == self_rid:\n",
    "        r_weight = omega\n",
    "    else:\n",
    "        r_weight = 1\n",
    "    if h in train_e2t:\n",
    "        normalize_h = len(train_e2t[h])\n",
    "        for type_h in train_e2t[h]:\n",
    "            if (type_h, r) in p_h:\n",
    "                df = p_h[(type_h, r)]\n",
    "                t_key = list(df.index)\n",
    "                t_value = df.values.reshape(-1)\n",
    "                entity2type[t][t_key] += t_value/h_count[(type_h, r)]/normalize_h * r_weight\n",
    "\n",
    "    if t in train_e2t:\n",
    "        normalize_t = len(train_e2t[t])\n",
    "        for type_t in train_e2t[t]:\n",
    "            if (r, type_t) in p_t:\n",
    "                df = p_t[(r, type_t)]\n",
    "                h_key = list(df.index)\n",
    "                h_value = df.values.reshape(-1)\n",
    "                entity2type[h][h_key] += h_value/t_count[(r, type_t)]/normalize_t * r_weight\n",
    "\n",
    "    if (i+1)%10000 == 0:\n",
    "        print(i+1,\"triples have been calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results of validation： (0.31354455659943675, 0.23650529960925618, 0.33606308055874023, 0.46318350920704743)\n",
      "results of test： (0.3153613580673738, 0.2382690058479532, 0.3373333333333333, 0.46362573099415205)\n"
     ]
    }
   ],
   "source": [
    "dev_evaluation = evaluation('dev')\n",
    "test_evaluation = evaluation('test')\n",
    "        \n",
    "print(\"results of validation：\",dev_evaluation)\n",
    "print(\"results of test：\",test_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
